{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\\n\\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\\n\\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"wiki.txt\") as f:\n",
    "  wiki = f.read()\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wiki.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Year Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*. A direct \"\\d{4}\" pattern identification would extract the first four digits of tokens like 123456, etc. See the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022', '2500']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.compile(r\"\\d{4}\").findall(\"The total cost in 2022 was 250072$.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach to first extract 4 digit numbers surrounded by non-digits and then extract those four digits will make sure that the extraction is at least not a part of a non-4 digit number token. Anyhow, we cannot guarantee that the four digit number is a year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years with the surrounding characters:   1950s  1950,  1954   1966,  1980s  1960s  1964   1966.\n",
      "Years extracted:  ['1950', '1950', '1954', '1966', '1980', '1960', '1964', '1966']\n"
     ]
    }
   ],
   "source": [
    "year_context_pattern = re.compile(r\"\\D\\d{4}\\D\") # pattern to identify a year - 4 consecutive digits surrounded by non-digit characters\n",
    "extracted_year_contexts = \" \".join(year_context_pattern.findall(wiki))\n",
    "print(\"Years with the surrounding characters: \", extracted_year_contexts)\n",
    "year_pattern = re.compile(r\"\\d{4}\") \n",
    "extracted_years = year_pattern.findall(extracted_year_contexts)\n",
    "print(\"Years extracted: \", extracted_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the history of nlp generally started in the 1950s, although work can be found from earlier periods. in 1950, alan turing published an article titled \"computing machinery and intelligence\" which proposed what is now called the turing test as a criterion of intelligence.\\n\\nthe georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.[2] however, real progress was much slower, and after the alpac report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\\n\\nsome notably successful nlp systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\".'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"-\" shall be replaced with single space character and not empty string to avoid words like naturallanguage & tenyearlong\n",
    "wiki = re.sub(r\"-\", \" \", wiki)\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the history of nlp generally started in the 1950s, although work can be found from earlier periods. in 1950, alan turing published an article titled \"computing machinery and intelligence\" which proposed what is now called the turing test as a criterion of intelligence.\\n\\nthe georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem. however, real progress was much slower, and after the alpac report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\\n\\nsome notably successful nlp systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\".'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the citation [n]\n",
    "wiki = re.sub(r\"[\\\\[]\\d[\\]]\", \"\", wiki)\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the history of nlp generally started in the 1950s, although work can be found from earlier periods. in 1950, alan turing published an article titled \"computing machinery and intelligence\" which proposed what is now called the turing test as a criterion of intelligence.  the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem. however, real progress was much slower, and after the alpac report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.  some notably successful nlp systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\".'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing new-line character \\n with \" \" (any extra space resulting due to this will be automatically removed while tokenized)\n",
    "wiki = re.sub(r\"[\\n]\", \" \", wiki)\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the history of nlp generally started in the 1950s although work can be found from earlier periods in 1950 alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as a criterion of intelligence  the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english the authors claimed that within three or five years machine translation would be a solved problem however real progress was much slower and after the alpac report in 1966 which found that ten year long research had failed to fulfill the expectations funding for machine translation was dramatically reduced little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed  some notably successful nlp systems developed in the 1960s were shrdlu a natural language system working in restricted blocks worlds with restricted vocabularies and eliza a simulation of a rogerian psychotherapist written by joseph weizenbaum between 1964 and 1966 using almost no information about human thought or emotion eliza sometimes provided a startlingly human like interaction when the patient exceeded the very small knowledge base eliza might provide a generic response for example responding to my head hurts with why do you say your head hurts'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing any non-alphanumeric and non-space characters with empty string\n",
    "wiki = re.sub(\"[^\\w\\s]\", \"\", wiki)\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Meenakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'history', 'of', 'nlp', 'generally', 'started', 'in', 'the', '1950s', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', 'in', '1950', 'alan', 'turing', 'published', 'an', 'article', 'titled', 'computing', 'machinery', 'and', 'intelligence', 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence', 'the', 'georgetown', 'experiment', 'in', '1954', 'involved', 'fully', 'automatic', 'translation', 'of', 'more', 'than', 'sixty', 'russian', 'sentences', 'into', 'english', 'the', 'authors', 'claimed', 'that', 'within', 'three', 'or', 'five', 'years', 'machine', 'translation', 'would', 'be', 'a', 'solved', 'problem', 'however', 'real', 'progress', 'was', 'much', 'slower', 'and', 'after', 'the', 'alpac', 'report', 'in', '1966', 'which', 'found', 'that', 'ten', 'year', 'long', 'research', 'had', 'failed', 'to', 'fulfill', 'the', 'expectations', 'funding', 'for', 'machine', 'translation', 'was', 'dramatically', 'reduced', 'little', 'further', 'research', 'in', 'machine', 'translation', 'was', 'conducted', 'until', 'the', 'late', '1980s', 'when', 'the', 'first', 'statistical', 'machine', 'translation', 'systems', 'were', 'developed', 'some', 'notably', 'successful', 'nlp', 'systems', 'developed', 'in', 'the', '1960s', 'were', 'shrdlu', 'a', 'natural', 'language', 'system', 'working', 'in', 'restricted', 'blocks', 'worlds', 'with', 'restricted', 'vocabularies', 'and', 'eliza', 'a', 'simulation', 'of', 'a', 'rogerian', 'psychotherapist', 'written', 'by', 'joseph', 'weizenbaum', 'between', '1964', 'and', '1966', 'using', 'almost', 'no', 'information', 'about', 'human', 'thought', 'or', 'emotion', 'eliza', 'sometimes', 'provided', 'a', 'startlingly', 'human', 'like', 'interaction', 'when', 'the', 'patient', 'exceeded', 'the', 'very', 'small', 'knowledge', 'base', 'eliza', 'might', 'provide', 'a', 'generic', 'response', 'for', 'example', 'responding', 'to', 'my', 'head', 'hurts', 'with', 'why', 'do', 'you', 'say', 'your', 'head', 'hurts']\n"
     ]
    }
   ],
   "source": [
    "regexpTokenized = RegexpTokenizer(pattern='\\w+|\\$[\\d\\.]+|\\S+').tokenize(wiki)\n",
    "print(regexpTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'history', 'of', 'nlp', 'generally', 'started', 'in', 'the', '1950s', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', 'in', '1950', 'alan', 'turing', 'published', 'an', 'article', 'titled', 'computing', 'machinery', 'and', 'intelligence', 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence', 'the', 'georgetown', 'experiment', 'in', '1954', 'involved', 'fully', 'automatic', 'translation', 'of', 'more', 'than', 'sixty', 'russian', 'sentences', 'into', 'english', 'the', 'authors', 'claimed', 'that', 'within', 'three', 'or', 'five', 'years', 'machine', 'translation', 'would', 'be', 'a', 'solved', 'problem', 'however', 'real', 'progress', 'was', 'much', 'slower', 'and', 'after', 'the', 'alpac', 'report', 'in', '1966', 'which', 'found', 'that', 'ten', 'year', 'long', 'research', 'had', 'failed', 'to', 'fulfill', 'the', 'expectations', 'funding', 'for', 'machine', 'translation', 'was', 'dramatically', 'reduced', 'little', 'further', 'research', 'in', 'machine', 'translation', 'was', 'conducted', 'until', 'the', 'late', '1980s', 'when', 'the', 'first', 'statistical', 'machine', 'translation', 'systems', 'were', 'developed', 'some', 'notably', 'successful', 'nlp', 'systems', 'developed', 'in', 'the', '1960s', 'were', 'shrdlu', 'a', 'natural', 'language', 'system', 'working', 'in', 'restricted', 'blocks', 'worlds', 'with', 'restricted', 'vocabularies', 'and', 'eliza', 'a', 'simulation', 'of', 'a', 'rogerian', 'psychotherapist', 'written', 'by', 'joseph', 'weizenbaum', 'between', '1964', 'and', '1966', 'using', 'almost', 'no', 'information', 'about', 'human', 'thought', 'or', 'emotion', 'eliza', 'sometimes', 'provided', 'a', 'startlingly', 'human', 'like', 'interaction', 'when', 'the', 'patient', 'exceeded', 'the', 'very', 'small', 'knowledge', 'base', 'eliza', 'might', 'provide', 'a', 'generic', 'response', 'for', 'example', 'responding', 'to', 'my', 'head', 'hurts', 'with', 'why', 'do', 'you', 'say', 'your', 'head', 'hurts']\n"
     ]
    }
   ],
   "source": [
    "wordTokenized = word_tokenize(wiki)\n",
    "print(wordTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexpTokenized == wordTokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, both the tokenising methods produced the same list of tokens. \n",
    "\n",
    "One advantage of RegexpTokenizer() is that we can specify either the pattern of tokens or of the delimiter using regular expressions, thereby customise the tokenisation process as according to the domain of the text or the requirement. This cannot be done in case of word_tokenize().\n",
    "\n",
    "*Note*. If the above two tokenisations were done without the removal of punctuations, the following nuances can be observed.\n",
    "* RegexpTokenizer() removed double quotations while word_tokenize() took them as separate tokens.\n",
    "* RegexpTokenizer() kept anything immediately followed by a period as such while word_tokenize() split the period and the following terms as separate tokens.\n",
    "> * Eg. RegexpTokenizer() kept '.[2]' as '.[2]' itself while word_tokenize() split them into '.', '[', '2', ']'\n",
    "* Hyphenated words were tokenized as follows.\n",
    "> * Example 1: RegexpTokenizer() tokenised 'ten-year-long' as 'ten' & '-year-long' while word_tokenize() tokenised it as 'ten-year-long' itself. \n",
    "> * Example 2: RegexpTokenizer() tokenised 'natural-language' as 'natural' & '-language' while word_tokenize() tokenised it as 'natural-language' itself. \n",
    "\n",
    "Anyhow, since RegexpTokenizer()'s tokenization/delimiter pattern can be customised, these differences can be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_words = wordTokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"mightn't\", 'between', 'yours', 'mightn', 'by', 't', 'before', 'shouldn', 'as', 'those', 'but', 'about', \"aren't\", 'whom', 'll', 'weren', 'under', \"wouldn't\", 'very', 'for', 'hasn', 'ain', 'down', 'only', 'now', 'their', \"she's\", 'there', 'don', 'of', 'when', 'you', 'm', 'has', 'am', 'just', 'we', 'where', 'ours', \"hadn't\", 'were', 'they', 'other', \"doesn't\", 'that', \"that'll\", 'aren', 'on', 'o', 'all', 's', 'was', 'will', 'until', 'it', \"hasn't\", 'my', 'from', 'are', 'themselves', 'the', \"you'll\", 'against', 'doing', \"needn't\", 'himself', 'them', 'again', 're', 'both', \"won't\", 'such', 'which', \"haven't\", 'couldn', 'didn', 'above', 'does', 'not', 'i', \"wasn't\", 'these', 'up', 'hadn', 'me', 'is', 'so', 'your', 'theirs', 'do', 'be', 'have', 'through', \"you've\", 'haven', 'because', 'and', 'ma', 'our', \"isn't\", \"it's\", 'or', 'ourselves', 'this', 'having', 'further', 'won', 'being', 'shan', 'what', 'itself', 'yourselves', 've', 'below', 'into', 'herself', 'had', 'here', 'more', 'a', 'after', 'who', 'most', \"mustn't\", 'out', 'why', 'own', 'y', \"shan't\", 'wouldn', 'to', 'than', 'its', 'isn', 'an', \"didn't\", 'myself', 'he', 'been', 'should', 'with', \"you're\", 'she', 'too', 'how', \"shouldn't\", 'while', 'once', 'during', 'then', 'nor', \"don't\", \"weren't\", 'can', 'few', 'did', 'each', 'in', 'hers', 'some', 'her', 'd', \"should've\", \"couldn't\", 'mustn', 'over', 'any', 'doesn', 'same', 'yourself', 'wasn', 'his', \"you'd\", 'at', 'needn', 'off', 'him', 'no', 'if'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Meenakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwds = set(stopwords.words('english'))\n",
    "print(stopwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'about', 'after', 'an', 'and', 'as', 'be', 'between', 'by',\n",
       "       'can', 'do', 'for', 'from', 'further', 'had', 'in', 'into', 'is',\n",
       "       'more', 'my', 'no', 'now', 'of', 'or', 'some', 'than', 'that',\n",
       "       'the', 'to', 'until', 'very', 'was', 'were', 'what', 'when',\n",
       "       'which', 'why', 'with', 'you', 'your'], dtype='<U7')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the words that would get removed as a stopword from wiki\n",
    "import numpy as np\n",
    "np.unique([word for word in wiki_words if word in stopwds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1950' '1950s' '1954' '1960s' '1964' '1966' '1980s' 'alan' 'almost'\n",
      " 'alpac' 'although' 'article' 'authors' 'automatic' 'base' 'blocks'\n",
      " 'called' 'claimed' 'computing' 'conducted' 'criterion' 'developed'\n",
      " 'dramatically' 'earlier' 'eliza' 'emotion' 'english' 'example' 'exceeded'\n",
      " 'expectations' 'experiment' 'failed' 'first' 'five' 'found' 'fulfill'\n",
      " 'fully' 'funding' 'generally' 'generic' 'georgetown' 'head' 'history'\n",
      " 'however' 'human' 'hurts' 'information' 'intelligence' 'interaction'\n",
      " 'involved' 'joseph' 'knowledge' 'language' 'late' 'like' 'little' 'long'\n",
      " 'machine' 'machinery' 'might' 'much' 'natural' 'nlp' 'notably' 'patient'\n",
      " 'periods' 'problem' 'progress' 'proposed' 'provide' 'provided'\n",
      " 'psychotherapist' 'published' 'real' 'reduced' 'report' 'research'\n",
      " 'responding' 'response' 'restricted' 'rogerian' 'russian' 'say'\n",
      " 'sentences' 'shrdlu' 'simulation' 'sixty' 'slower' 'small' 'solved'\n",
      " 'sometimes' 'started' 'startlingly' 'statistical' 'successful' 'system'\n",
      " 'systems' 'ten' 'test' 'thought' 'three' 'titled' 'translation' 'turing'\n",
      " 'using' 'vocabularies' 'weizenbaum' 'within' 'work' 'working' 'worlds'\n",
      " 'would' 'written' 'year' 'years']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'history nlp generally started 1950s although work found earlier periods 1950 alan turing published article titled computing machinery intelligence proposed called turing test criterion intelligence georgetown experiment 1954 involved fully automatic translation sixty russian sentences english authors claimed within three five years machine translation would solved problem however real progress much slower alpac report 1966 found ten year long research failed fulfill expectations funding machine translation dramatically reduced little research machine translation conducted late 1980s first statistical machine translation systems developed notably successful nlp systems developed 1960s shrdlu natural language system working restricted blocks worlds restricted vocabularies eliza simulation rogerian psychotherapist written joseph weizenbaum 1964 1966 using almost information human thought emotion eliza sometimes provided startlingly human like interaction patient exceeded small knowledge base eliza might provide generic response example responding head hurts say head hurts'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the tokens for any additional stopwords\n",
    "print(np.unique([word for word in wiki_words if word not in stopwds]))\n",
    "' '.join([word for word in wiki_words if word not in stopwds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 'no' from the stopwords list since \"using almost no information\" and \"using almost information\" have different meanings.\n",
    "\n",
    "Add 'much' & 'however' to the stopwords list since \"*however* progress was *much* slower\" would mean almost the same without these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwds.remove('no')\n",
    "stopwds.update({'much', 'however'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "wiki_words = [word for word in wiki_words if word not in stopwds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['history', 'nlp', 'generally', 'started', '1950s', 'although', 'work', 'found', 'earlier', 'periods', '1950', 'alan', 'turing', 'published', 'article', 'titled', 'computing', 'machinery', 'intelligence', 'proposed', 'called', 'turing', 'test', 'criterion', 'intelligence', 'georgetown', 'experiment', '1954', 'involved', 'fully', 'automatic', 'translation', 'sixty', 'russian', 'sentences', 'english', 'authors', 'claimed', 'within', 'three', 'five', 'years', 'machine', 'translation', 'would', 'solved', 'problem', 'real', 'progress', 'slower', 'alpac', 'report', '1966', 'found', 'ten', 'year', 'long', 'research', 'failed', 'fulfill', 'expectations', 'funding', 'machine', 'translation', 'dramatically', 'reduced', 'little', 'research', 'machine', 'translation', 'conducted', 'late', '1980s', 'first', 'statistical', 'machine', 'translation', 'systems', 'developed', 'notably', 'successful', 'nlp', 'systems', 'developed', '1960s', 'shrdlu', 'natural', 'language', 'system', 'working', 'restricted', 'blocks', 'worlds', 'restricted', 'vocabularies', 'eliza', 'simulation', 'rogerian', 'psychotherapist', 'written', 'joseph', 'weizenbaum', '1964', '1966', 'using', 'almost', 'no', 'information', 'human', 'thought', 'emotion', 'eliza', 'sometimes', 'provided', 'startlingly', 'human', 'like', 'interaction', 'patient', 'exceeded', 'small', 'knowledge', 'base', 'eliza', 'might', 'provide', 'generic', 'response', 'example', 'responding', 'head', 'hurts', 'say', 'head', 'hurts']\n"
     ]
    }
   ],
   "source": [
    "print(wiki_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "452b90bea1dc56bb7abd92e2828b74e7b1bfd708fd58372070ca854e487506c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
